{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Tree\n",
    "### MC_note\n",
    "##### attributes\n",
    "- state, type of list ???\n",
    "- edges, type of list\n",
    "- N, type of int\n",
    "    - number of times, the note been explored\n",
    "- id, type of string\n",
    "    - identify number\n",
    "\n",
    "##### method\n",
    "- add_edge()\n",
    "- add_edges()\n",
    "- is_leaf()\n",
    "    - return bool\n",
    "- get_N()\n",
    "- get_state()\n",
    "\n",
    "### MC_edge\n",
    "##### attributes\n",
    "- action, type of string\n",
    "- in_node, type of MC_note\n",
    "- out_node, type of MC_note\n",
    "- N, type of int\n",
    "- id, type of string\n",
    "- Q, type of double\n",
    "- U, action bonus\n",
    "- W, type of double\n",
    "- P, type of double\n",
    "\n",
    "##### methods\n",
    "- get_in_node()\n",
    "- get_out_node()\n",
    "- get_state()\n",
    "    - return (Q, W, N, P)\n",
    "- get_value()\n",
    "- recalculate_value()\n",
    "- set_q(q)\n",
    "\n",
    "### MC_tree\n",
    "##### attributes\n",
    "- root, type of mc_node\n",
    "- path, type of list\n",
    "- tree, dictionary\n",
    "- cpuct\n",
    "\n",
    "##### methods\n",
    "- add_node()\n",
    "- back_fill()\n",
    "- expansion()\n",
    "- selection()\n",
    "- set_root()\n",
    "- simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_node():\n",
    "    def __init__(self, state, N = 0):\n",
    "        self.state = state\n",
    "        self.id = self.state.get_id()\n",
    "        self.N = N\n",
    "        self.edges = []\n",
    "    def add_edge(e):\n",
    "        self.edges.append(e)\n",
    "    def add_edges(es):\n",
    "        self.edges.extend(es)\n",
    "    def is_leaf(self):\n",
    "        if len(self.edges) == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "    def get_N(self):\n",
    "        return self.N\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "class MC_edge():\n",
    "    def __init__(self, action, in_node, out_node, priori):\n",
    "        self.action = action\n",
    "        self.in_node = in_node\n",
    "        self.out_node = out_node\n",
    "        self.id = in_node.get_id() + '-' + out_node.get_id()\n",
    "        self.N = 0\n",
    "        self.W = 0\n",
    "        self.Q = 0\n",
    "        self.U = priori\n",
    "        self.P = priori # 获胜的概率，由网络得到\n",
    "        self.value = self.Q + self.U\n",
    "    def get_in_node(self):\n",
    "        return self.in_node\n",
    "    def get_out_node(self):\n",
    "        return self.out_node\n",
    "    def get_action(self):\n",
    "        return self.action\n",
    "    def get_state(self):\n",
    "        return (self.Q, self.U, self.W, self.N, self.P)\n",
    "    def get_value(self):\n",
    "        # return the value of the edge, the one here is just for test --------------------\n",
    "        # should add an new attribute value for the class edge???? -----------------------\n",
    "        return self.value\n",
    "    def set_Q(q):\n",
    "        self.Q = q\n",
    "\n",
    "class MC_tree():\n",
    "    def __init__(self, root_state, cpuct, logger = None):\n",
    "        self.root = MC_node(root_state)\n",
    "        self.path = []\n",
    "        self.tree = {}\n",
    "        self.cpuct = cpuct\n",
    "        self.add_node(self.root)\n",
    "    def add_node(self, node):\n",
    "        if node.get_id() not in self.tree.keys():\n",
    "            self.tree[node.get_id()] = node\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"node exist\")\n",
    "            return 0\n",
    "    def back_fill(self, value):\n",
    "        # update all the node and edge in the path\n",
    "        #   - N add 1 for each edge\n",
    "        #   - N add 1 for each Node \n",
    "        #   - recalculate the Q and W value of edge according to the new N value.\n",
    "        for edge in self.path:\n",
    "            edge.N += 1\n",
    "            edge.in_node.N += 1\n",
    "            edge.W = e.W + value\n",
    "            edge.Q = edge.W/edge.N\n",
    "            edge.U = (self.cpuct * math.sqrt(edge.in_node.N) * edge.priori)/ edge.N\n",
    "            edge.value = edge.Q + edge.U\n",
    "    def expansion(self, leaf_state, actions, states, values):\n",
    "        # values here is just the output of the p_net, \n",
    "        # still not sure what kind of infos should be setted to the edge here ----------------\n",
    "        # TODO\n",
    "        # get the leaf node\n",
    "        for action, state, value in zip(actions, states, values):\n",
    "            out_node = MC_node(state)\n",
    "            edge = MC_edge(action, leaf, out_node, value)\n",
    "            leaf.add_edge(edge)\n",
    "    def selection(self, current_state):\n",
    "        # move to the leaf and save the path  \n",
    "        self.path = []\n",
    "        current_node = MC_node(current_state)\n",
    "        if current_node.get_id() not in self.tree.keys():\n",
    "            return current_node, self.path\n",
    "        else:\n",
    "            while not current_node.is_leaf():\n",
    "                tmp_edge = max(current_node.edges, key= lambda x: x.get_value())\n",
    "                current_node = tmp_edge.get_out_node()\n",
    "                self.path.append(tmp_edge)\n",
    "            return current_node, self.path\n",
    "    def set_root(self, node):\n",
    "        self.root = node\n",
    "        path = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "### Agent\n",
    "##### attributes\n",
    "- game\n",
    "- root\n",
    "- mct\n",
    "\n",
    "\n",
    "##### methods\n",
    "- evaluate_leaf_state()\n",
    "- get_suggestion()\n",
    "- simulation()\n",
    "- take_action()\n",
    "- train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#应该区分root和current state\n",
    "class Agent():\n",
    "    def __init__(self, game, cpuct, logger = None):\n",
    "        self.game = game\n",
    "        self.root_state = game.get_current_state()\n",
    "        self.mct = MC_tree(self.root_state, cpuct)\n",
    "        #self.p_net = p_net\n",
    "    def episode(self):\n",
    "        leaf,_ = self.mct.selection(self.root_state)\n",
    "        # test if the state of leaf staying in the done position\n",
    "        #  - yes, get the reward and fill back\n",
    "        #  - no, do the expansion of the leaf\n",
    "        if self.game.is_done(leaf.get_state()):\n",
    "            end_reword == self.game.get_end_reword(state)\n",
    "        else:\n",
    "            available_actions, states, values = self.evaluate_leaf_state(leaf.get_state())\n",
    "            self.mct.expansion(leaf, available_actions, states, values)\n",
    "            end_reword = self.simulation(leaf.get_state())\n",
    "        self.mct.backfill(end_reword)\n",
    "    def evaluate_leaf_state(self, state_leaf):\n",
    "        #state_leaf = leaf.get_state()\n",
    "        available_actions = self.game.get_available_actions(state_leaf)\n",
    "        states = []\n",
    "        values = []\n",
    "        for action in available_actions:\n",
    "            new_state = self.simulate_action(state_leaf, action)\n",
    "            value_new_state = self.game.get_reward(new_state)\n",
    "            states.append(new_state)\n",
    "            values.append(value_new_state)\n",
    "        return available_actions, states, values\n",
    "    def get_suggestion():\n",
    "        leaf, path = self.mct.selection(self.root_state)\n",
    "        path = [e.get_action() for e in path]\n",
    "        if self.game.is_done(leaf.get_state()):\n",
    "            return path\n",
    "        else:\n",
    "            current_state = leaf.get_state()\n",
    "            while not self.game.is_done(current_state):\n",
    "                actions, states, values = evaluate_leaf_state(current_state)\n",
    "                action, current_state, _ = max(zip(actions, states, values), key = lambda x: x[2])\n",
    "                path.append(action)\n",
    "            return path\n",
    "    def simulate_action(self, cuu_state, action):\n",
    "        return self.game.simulate_action(cuu_state, action)\n",
    "    def simulation(self, tmp_state):\n",
    "        while not self.game.is_done(tmp_state):\n",
    "            _, states, values = evaluate_leaf_state(tmp_state)\n",
    "            tmp_state, _ = max(zip(states, values), key = lambda x: x[1])\n",
    "        return self.game.get_end_reword()\n",
    "    def take_action(self, action):\n",
    "        # return the state after taking the action.\n",
    "        self.game.take_action()\n",
    "        self.root = self.game.get_current_state()\n",
    "    def train_network():\n",
    "        # could the episode data used to train the model again??? ---> no\n",
    "        # shoud i add another model to the to predict the result and the simulation step just use random choose\n",
    "        # it can enfast the process and verringt the error\n",
    "        # then i can use the mcst to generate some data, and calculate the real answer. \n",
    "        # then used these data to train the model again.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game\n",
    "### Game(abstract)\n",
    "##### abstractmethods\n",
    "- get_available_actions(self, state)\n",
    "- get_current_state(self)\n",
    "- get_end_reword(self)\n",
    "- is_done(self, state)\n",
    "- restore_game(self)\n",
    "- take_action(self, action)\n",
    "### TS\n",
    "##### attributes\n",
    "- lens\n",
    "- thread\n",
    "- value_network\n",
    "- restore_game\n",
    "##### methods:\n",
    "- get_available_actions(self, state)\n",
    "- get_current_state(self)\n",
    "- get_end_reword(self)\n",
    "- is_done(self, state)\n",
    "- restore_game(self)\n",
    "- take_action(self, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(metaclass = ABCMeta):\n",
    "    @abstractmethod\n",
    "    def get_available_actions(self, state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          state, can be any kind of type\n",
    "        output:\n",
    "          actions: list of action, action should be string\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def get_current_state(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def is_done(self, state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          state\n",
    "        output:\n",
    "          out: if the game is done return the result, else return 0\n",
    "        \"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def restore_game(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def simulate_action(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def take_action(self):\n",
    "        pass\n",
    "\n",
    "class TS(Game):\n",
    "    def __init__(self, lens, thread, value_network):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          lens, type of int\n",
    "              total number of parts\n",
    "          thread, type of float\n",
    "              value used to decide, weather the seq is good or not\n",
    "          value_network, type of pkl\n",
    "              neural network, that used to output the finalpunkt.\n",
    "        \"\"\"\n",
    "        super(TS, self).__init__()\n",
    "        self.actions = ['1', '2', '3', '4', '5', '6']\n",
    "        self.lens = lens\n",
    "        self.thread = thread\n",
    "        self.value_network = value_network   \n",
    "        self.restore_game()\n",
    "        self.setup_lookup()\n",
    "    def get_available_actions(self, state):\n",
    "        return self.actions\n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "    def get_original_state(self):\n",
    "        return State([])\n",
    "    def get_reward(self, state, is_done = False):\n",
    "        inp_nn = self._transform_state_to_input(state)\n",
    "        out_nn = self.value_network(state)\n",
    "        score = self._transform_output_to_value(out_nn)\n",
    "        if not is_done:\n",
    "            return score\n",
    "        if score <= thread:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    def _transform_state_to_input(self, state):\n",
    "            #???\n",
    "            out = []\n",
    "            for action in state.state:\n",
    "                out.append(self.lookup[action])\n",
    "            return torch.FloatTensor(out).unsqueeze(0)\n",
    "    def _transform_ouput_to_value(self, output):\n",
    "            #???\n",
    "            return output.mean()\n",
    "    def is_done(self, state):\n",
    "        if len(state) == self.lens:\n",
    "            # done\n",
    "            return True\n",
    "        else:\n",
    "            # not jet\n",
    "            return False\n",
    "    def restore_game(self):\n",
    "        self.current_state = self.get_original_state()\n",
    "    def simulate_action(self, cuu_state, action):\n",
    "        cuu_state.take_action(action)\n",
    "        return cuu_state\n",
    "    def setup_lookup(self):\n",
    "        self.lookup = {'1': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       '2': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       '3': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                       '4': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                       '5': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                       '6': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\n",
    "    def take_action(self, action):\n",
    "        # change state after taking the action\n",
    "        self.current_state.take_action(action)\n",
    "                \n",
    "class State():\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.id = str(self.state)\n",
    "    def __len__(self):\n",
    "        return len(self.state)\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "    def take_action(self, action):\n",
    "        self.state.append(action)\n",
    "        self.id = str(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/rnn_predict.pkl')\n",
    "game = TS(50, 4.0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(game, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose(): argument 'input' (position 1) must be Tensor, not State",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a6367093ee84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-61c2bbbb4395>\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mend_reword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_end_reword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mavailable_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_leaf_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mend_reword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-61c2bbbb4395>\u001b[0m in \u001b[0;36mevaluate_leaf_state\u001b[0;34m(self, state_leaf)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mvalue_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_new_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-a3c529c3203e>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self, state, is_done)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0minp_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_state_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_output_to_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/data_mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5190cd40e976>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# input of the rnn (seq_len, batch, input_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdata_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# run rnn, it has two output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose(): argument 'input' (position 1) must be Tensor, not State"
     ]
    }
   ],
   "source": [
    "agent.episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bild the model, loss and data class\n",
    "class TS_rnn(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    scores for each piece\n",
    "    input:\n",
    "        tensor size of (batch_size, seq_len, num_dim)\n",
    "    output:\n",
    "        tensor size of (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TS_rnn, self).__init__()\n",
    "        #change the structure of the network\n",
    "        num_inp = 8\n",
    "        num_hidden = 64\n",
    "        self.rnn = torch.nn.LSTM(input_size = num_inp, hidden_size = num_hidden, num_layers = 2)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(num_hidden, 16),\n",
    "                torch.nn.Dropout(),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(16, 1)\n",
    "                )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # input of the rnn (seq_len, batch, input_size)\n",
    "        data_in = torch.transpose(inp, 0, 1)\n",
    "        # run rnn, it has two output\n",
    "        out_rnn, _ = self.rnn(data_in)\n",
    "        out_rnn = torch.transpose(out_rnn, 0, 1) # (batch_size, seq_len, num_dim)\n",
    "        # rnn the mlp\n",
    "        batch_size, seq_len, num_dim = out_rnn.shape\n",
    "        out = []\n",
    "        for i in range(seq_len):\n",
    "            tmp = self.mlp(out_rnn[:, i,:])\n",
    "            out.append(tmp)\n",
    "        # now out is list of (batch_size, 1), combine the items in the list to get the output with size (batch_size, seq_len)\n",
    "        out = torch.cat(out, 1)\n",
    "        return out.squeeze()\n",
    "\n",
    "class TS_rnn2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    scores only for the whole task\n",
    "    input:\n",
    "        tensor size of (batch_size, seq_len, num_dim)\n",
    "    output:\n",
    "        tensor size of (batch_size)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TS_rnn2, self).__init__()\n",
    "        #change the structure of the network\n",
    "        num_inp = 8\n",
    "        num_hidden = 64\n",
    "        self.rnn = torch.nn.LSTM(input_size = num_inp, hidden_size = num_hidden, num_layers = 2)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(num_hidden, 64),\n",
    "                torch.nn.Dropout(),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(64, 1)\n",
    "                )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # input of the rnn (seq_len, batch, input_size)\n",
    "        data_in = torch.transpose(inp, 0, 1)\n",
    "        # run rnn, it has two output\n",
    "        out_rnn, _ = self.rnn(data_in)\n",
    "        out_rnn = torch.transpose(out_rnn, 0, 1) # (batch_size, seq_len, num_dim)\n",
    "        # only use the last output\n",
    "        out_rnn = out_rnn[:, -1, :].squeeze()\n",
    "        # rnn the mlp\n",
    "        out = self.mlp(out_rnn)\n",
    "        return out.squeeze()\n",
    "    \n",
    "class PDLoss(torch.nn.Module):\n",
    "    def __init__(self, p = 2):\n",
    "        super(PDLoss, self).__init__()\n",
    "        self.pd = torch.nn.PairwiseDistance(p)\n",
    "\n",
    "    def forward(self, o, t):\n",
    "        # out: (batch_size, 1)\n",
    "        out = self.pd(o, t)\n",
    "        return out.mean()\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    data class for TS_rnn\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.data = {}\n",
    "        self.data['train_x'] = self.add_file(x)\n",
    "        self.data['train_y'] = self.add_file(y)[:, :, -1] # use the first metric tempately\n",
    "        assert(len(self.data['train_x']) == len(self.data['train_y']))\n",
    "        self.len = len(self.data['train_x'])\n",
    "\n",
    "    def add_file(self, path):\n",
    "        return torch.from_numpy(np.load(path))\n",
    "\n",
    "    def add_scores(self, path):\n",
    "        return torch.FloatTensor([float(li.rstrip('\\n')) for li in open(path)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data['train_x'][index],\n",
    "                self.data['train_y'][index])\n",
    "\n",
    "class Data2:\n",
    "    \"\"\"\n",
    "    data class for TS_rnn2\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.data = {}\n",
    "        self.data['train_x'] = self.add_file(x)\n",
    "        self.data['train_y'] = self.add_file(y)[:, :, -1] # use the first metric tempately\n",
    "        self.data['train_y'] = torch.mean(self.data['train_y'], 1)\n",
    "        assert(len(self.data['train_x']) == len(self.data['train_y']))\n",
    "        self.len = len(self.data['train_x'])\n",
    "\n",
    "    def add_file(self, path):\n",
    "        return torch.from_numpy(np.load(path))\n",
    "\n",
    "    def add_scores(self, path):\n",
    "        return torch.FloatTensor([float(li.rstrip('\\n')) for li in open(path)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data['train_x'][index],\n",
    "                self.data['train_y'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the test function\n",
    "def test_model(dl_test, model, loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    counter = 0\n",
    "    for batch_idx, dat in enumerate(dl_test):\n",
    "        counter += 1\n",
    "        # codes to be changed\n",
    "        inp, target = dat\n",
    "        out = model(inp)\n",
    "        lo = loss(out, target.squeeze())\n",
    "        test_loss += lo.data\n",
    "    return test_loss/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
